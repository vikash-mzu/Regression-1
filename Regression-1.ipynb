{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9c6c3-9c89-4c15-a9d7-76c78910bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Difference Between Simple Linear Regression and Multiple Linear Regression\n",
    "Simple Linear Regression:\n",
    "•\tDefinition: It models the relationship between a single independent variable (predictor) and a dependent variable (response) using a straight line.\n",
    "•\tEquation: y=β0+β1x+ϵy = \\beta_0 + \\beta_1 x + \\epsilony=β0+β1x+ϵ\n",
    "•\tExample: Predicting a person’s weight (dependent variable) based on their height (independent variable).\n",
    "Multiple Linear Regression:\n",
    "•\tDefinition: It models the relationship between two or more independent variables and a dependent variable using a hyperplane in multi-dimensional space.\n",
    "•\tEquation: y=β0+β1x1+β2x2+⋯+βnxn+ϵy = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\epsilony=β0+β1x1+β2x2+⋯+βnxn+ϵ\n",
    "•\tExample: Predicting a person’s weight based on height, age, and gender.\n",
    "Q2. Assumptions of Linear Regression\n",
    "Assumptions:\n",
    "1.\tLinearity: The relationship between the independent and dependent variables is linear.\n",
    "2.\tIndependence: Observations are independent of each other.\n",
    "3.\tHomoscedasticity: The variance of errors is constant across all levels of the independent variable(s).\n",
    "4.\tNormality of Errors: The residuals (errors) of the model are normally distributed.\n",
    "Checking Assumptions:\n",
    "•\tLinearity: Use scatter plots to visualize the relationship between predictors and the target variable.\n",
    "•\tIndependence: Examine residual plots and use statistical tests like the Durbin-Watson test.\n",
    "•\tHomoscedasticity: Plot residuals versus fitted values and look for constant variance.\n",
    "•\tNormality of Errors: Use Q-Q plots and statistical tests like the Shapiro-Wilk test.\n",
    "Q3. Interpreting Slope and Intercept\n",
    "Slope (β1\\beta_1β1):\n",
    "•\tDefinition: Indicates the change in the dependent variable for a one-unit change in the independent variable.\n",
    "•\tExample: If the slope of a regression line predicting weight from height is 0.5, it means that for each additional inch in height, weight increases by 0.5 pounds.\n",
    "Intercept (β0\\beta_0β0):\n",
    "•\tDefinition: The value of the dependent variable when the independent variable is zero.\n",
    "•\tExample: If the intercept in the weight-height regression is 50 pounds, it implies that a person with a height of zero inches would theoretically weigh 50 pounds (though this is not practically meaningful).\n",
    "Q4. Gradient Descent\n",
    "Definition:\n",
    "•\tA mathematical optimization algorithm used to minimize the cost function by iteratively moving towards the minimum value of the function.\n",
    "How It Works:\n",
    "•\tInitialization: Start with an initial guess for the model parameters.\n",
    "•\tUpdate: Adjust the parameters in the direction that reduces the cost function.\n",
    "•\tConvergence: Repeat the update steps until the changes in the cost function are sufficiently small or the maximum number of iterations is reached.\n",
    "Usage in Machine Learning:\n",
    "•\tGradient descent is used to optimize the parameters of models such as linear regression, neural networks, and other machine learning algorithms.\n",
    "Q5. Multiple Linear Regression Model\n",
    "Definition:\n",
    "•\tA model that predicts the value of a dependent variable based on multiple independent variables.\n",
    "Difference from Simple Linear Regression:\n",
    "•\tSimple Linear Regression: Uses one independent variable.\n",
    "•\tMultiple Linear Regression: Uses two or more independent variables.\n",
    "Q6. Multicollinearity in Multiple Linear Regression\n",
    "Definition:\n",
    "•\tMulticollinearity occurs when two or more independent variables in a multiple regression model are highly correlated, leading to instability in the estimation of coefficients.\n",
    "Detection:\n",
    "•\tVariance Inflation Factor (VIF): Calculate VIF for each independent variable; high VIF values indicate multicollinearity.\n",
    "•\tCorrelation Matrix: Examine pairwise correlations between predictors.\n",
    "Addressing Multicollinearity:\n",
    "•\tRemove Variables: Remove one of the highly correlated predictors.\n",
    "•\tCombine Variables: Combine correlated variables into a single predictor.\n",
    "•\tRegularization: Use techniques like Ridge or Lasso regression that penalize large coefficients.\n",
    "Q7. Polynomial Regression Model\n",
    "Definition:\n",
    "•\tA type of regression that models the relationship between the independent variable(s) and the dependent variable using polynomial functions.\n",
    "Difference from Linear Regression:\n",
    "•\tLinear Regression: Assumes a linear relationship.\n",
    "•\tPolynomial Regression: Can model non-linear relationships by including polynomial terms (e.g., x2x^2x2, x3x^3x3).\n",
    "Q8. Advantages and Disadvantages of Polynomial Regression\n",
    "Advantages:\n",
    "•\tCan Model Non-Linear Relationships: Captures more complex patterns in data.\n",
    "Disadvantages:\n",
    "•\tOverfitting: Higher-degree polynomials can overfit the training data.\n",
    "•\tComplexity: Increased complexity can make interpretation difficult and computation expensive.\n",
    "When to Use Polynomial Regression:\n",
    "•\tNon-Linear Relationships: When the relationship between variables is clearly non-linear.\n",
    "•\tData Trends: When you need to capture curvature in the data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
